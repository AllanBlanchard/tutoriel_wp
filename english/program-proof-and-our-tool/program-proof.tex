\levelThreeTitle{Ensure program reliability}


It is often difficult to ensure that our programs have only correct
behaviors. Moreover, it is already complex to establish good criteria
that make us confident enough to say that a program works correctly:



\begin{itemize}
\item
  beginners simply ``try'' to use their programs and consider that
  these programs work if they do not crash (which is not a really good
  indicator in C language),
\item
  more advanced developers establish some test cases for which they know
  the expected result and compare the output they obtain,
\item
  most companies establish complete test bases, that cover as much
  code as they can ; which are systematically executed on their code.
  Some of them apply test driven development,
\item
  in critical domains, such as aerospace, railway or armament, source
  code needs to be certified using standardized processes with very
  strict criteria about coding rules and code covering by the test.
\end{itemize}

In all these ways to ensure that a program produces only what we expect
it to produce, a word appears to be common: \emph{test}. We \emph{try}
different inputs in order to isolate cases that are problematic. We
provide inputs that we \emph{estimate to be representative} of the
actual use of the program (note that unexpected use cases are often not
considered whereas there are generally the most dangerous ones) and we
verify that the results we get are correct. But we cannot test
\emph{everything}. We cannot try \emph{every} combination of
\emph{every} possible input of a program. It is then quite hard to
choose good tests.


The goal of program proof is to ensure that, for any input provided to a
given program, if it conforms to the specification, then the program will
only well-behave. However, since we cannot test everything, we will
formally, mathematically, establish a proof that our software can only
exhibit specified behaviors, and that runtime-errors are not part of
these behaviors.



A very well-known quote from \externalLink{Dijkstra}{https://en.wikipedia.org/wiki/Edsger_Dijkstra}
precisely express the difference between test and proof:



\begin{Quotation}[Dijkstra]
Program testing can be used to show the presence of bugs, but never to show
their absence!
\end{Quotation}


\levelFourTitle{The developer's Holy Grail: the bug-free software}


Every time we read news about attacks on computer systems, or
viruses, or bugs leading to crashes in well known apps, there is always
the one same comment ``the bug-free/perfectly secure program does not
exist''. And, if this sentence is quite true, it is a bit misunderstood.


First, we do not really define what we mean by ``bug-free''. Creating
software always relies at least on two steps: we establish a specification
of what we expect from the program and then we produce the source code of
the program that must respect this specification. Moreover, we can add that
our programming language itself defines what is the correct way to use it.
On each of these aspects, an error can lead to the introduction of bugs that
we can classify into three categories:


\begin{itemize}
\item the program is not correct, or does not have a defined behavior,
      according to the language specification (for example, a program
      accesses out of the bounds of an array when searching the index of
      the minimal value of the array),
\item the program is not correct according to the specification that we
      defined for it (for example, we have defined that our program must
      find the index of the minimal value of an array, but in fact, due to
      an error, it does not check the last value of the array),
\item the specification does not exactly reflect what we had in mind for
      our function, and consequently, the program does not do what we really
      wanted (for example, we have defined that our function must find the
      index of the minimal value of the array, but we have not specified that
      if there are multiple ones, we need the position of the first one,
      because it seemed too straightforward, but consequently, this is not
      what the program does).
\end{itemize}


Each of these categories can affect the safety and/or the security of our
programs, but these two notions are not equivalent. Loosely speaking,
in security a malicious entity that can attack the system, while in safety, we
want to verify when used in a conform way, the system well behaves. Thus, safety
is a first before we can get security\footnote{Depending on the field you are
  working in, the term ``safety'' may have a very different sense. Namely, a
  \textit{safe} system must be a system that can never cause injuries to human
  beings. And thus in this case, the opposite applies: we can never have safety
  if security is not guaranteed before. In this tutorial, we keep the definition:
  ``safety = the program as a correct behavior when it is used in a correct way''}.


In this tutorial, we will show how we can prove that implementations
of our programs do not contain bugs related to the two first categories
defined above, meaning that they conform to:

\begin{itemize}
\item the specification of our language,
\item the specification that we have provided for them.
\end{itemize}


But what are the arguments of program
proof, compared to program testing? First, the proof is complete, it
cannot forget some corner case if the behavior is specified (program
test cannot be complete, being exhaustive would be far too costly).
Second, the obligation to formally specify with a logic formulation
requires to exactly understand what we have to prove about our program.



One could cynically say that program proof shows that ``the
implementation does not contain bugs that do not exist in the
specification'' as we do not treat the third category of bugs defined
above. But, well, knowing that ``the implementation does not contain bugs
that do not exist in the specification'' is already a big step compared to
knowing that ``the implementation does not contain too many bugs that do
not exist in the specification'', as it already correspond to two entire
categories of bugs that we rule out, bugs that can severely compromise the
safety and security of our programs. And moreover, there also exist approaches
to deal with the third category of bugs, allowing to analyze specifications
to find errors or under-specified behaviors. For
example, with model checking techniques, we can create an abstract model
from the specification and produce the set of states that can be reached
according to this model. By characterizing what is an error state, we
can determine if reachable states are error states.



\levelThreeTitle{A bit of context}


Formal methods, as we name them, allow in computer science to
rigorously, mathematically, reason about programs. There exist a lot of
formal methods that can take place at different levels from program
design to implementation, analysis and validation, and for all systems
that allow to manipulate information.



Here, we will focus on a method that allows to formally verify that our
programs have only correct behaviors. We will use tools that are able to
analyze a source code and to determine whether a program correctly
implements what we want to express. The analyzer we will use provides a
static analysis, that we can oppose to dynamic analysis.



In static analysis, the analyzed program is not executed. We reason on a
mathematical model of the states it can reach during its execution. On
the opposite, dynamic analyses such as program testing, require to
execute the analyzed source code. Note that there exist formal dynamic
analysis methods, for example automatic test generation, or code
monitoring techniques that allow to instrument a source code to verify
some properties about it during execution (correct memory use, for
example).



Talking about static analyses, the model we use can be more or less
abstract depending on the techniques, this is always an approximation of
possible states of the program. The more the approximation is precise,
the more the model is concrete, the more the approximation is vague, the
more it is abstract.



To illustrate the difference between concrete and abstract model, we can
have a look at the model of a simple chronometer. A very abstract model of
a chronometer could be the one presented here:



\image{model-en}


We have a model of the behavior of our chronometer with the different
states it can reach according to the different actions we can perform.
However, we have not modeled how these states are implemented in the
program (is this a C enumeration? a particular program point in the
source code?), nor how the computation of the elapsed time is done (a single
variable? multiple ones?). It would then be difficult to specify
properties about our program. We could add some information:


\begin{itemize}
\item State stopped at 0 : time = 0s
\item State running : time > 0s
\item State stopped : time > 0s
\end{itemize}



Which gives us a more concrete model but that is still not precise
enough to ask interesting questions like: ``is it possible for the program
to continue updating the time variable while the chronometer is on the Stopped
state?'', as we do not model how the time measurement is updated by the
chronometer.



On the opposite, with the source code of the program, we have a concrete
model of the chronometer. The source code expresses the behavior of the
chronometer since it will allow us to produce the executable. But this
is still not the more concrete model! For example, the executable in
machine code format, that we obtain after compilation, is far more
concrete than our program.



The more a model is concrete, the more it precisely describes the
behavior of our program. The source code more precisely describes the
behavior than our diagram, but it is less precise than the machine code.
However, the more the model is precise, the more it is difficult to have
a global view of the defined behavior. Our diagram is understandable in
the blink of an eye, the source code requires more time, and for the
executable \ldots{} Every single person that has already opened an
executable with a text editor by error knows that it is not really
pleasant to read\textsuperscript{\ref{footnote:1}}.



When we create an abstraction of a system, we approximate it, in order
to limit the knowledge we have about it and make our reasoning easier. A
constraint we must respect, if we want our analysis to be correct, is to
never under-approximate behaviors: we would risk to remove a behavior
that contains an error (and thus miss it during the analysis). However,
when we over-approximate the behaviors of the program, we can add
behaviors that cannot happen, and if we add too many of them, we could
not be able to prove that our program is correct, since some of them could be
faulty behaviors.



In our case, the model is quite concrete. Every type of instruction, of
control structure, is associated to a precise semantics, a model of its
behavior in a pure logic, mathematical, world. The logic we use here is
a variant of the Hoare logic, adapted to the C language and all its
complex subtleties (which makes this model concrete).

\footnotetext[1]{\label{footnote:1}
  There also exists formal methods which are
  interested in understanding how executable machine code work, for
  example in order to understand what malwares do or to detect security
  breaches introduced during compilation.}



\levelThreeTitle{Hoare triples}


Hoare logic is a program formalization method proposed by
\externalLink{Tony Hoare}{https://en.wikipedia.org/wiki/Charles_Antony_Richard_Hoare}
in 1969 in a paper entitled \emph{An Axiomatic Basis for Computer
Programming}. This method defines:



\begin{itemize}
\item   axioms, that are properties we admit, such as ``the skip action does
  not change the program state'',
\item   rules to reason about the different allowed combinations of actions,
  for example ``the skip action followed by the action A'' is equivalent
  to ``the action A''.
\end{itemize}


The behavior of the program is defined by what we call ``Hoare
triples'':




\begin{center}
$\{P\}\ C\ \{Q\}$
\end{center}


Where $P$ and $Q$ are predicates, logic formulas that express
properties about the memory at particular program points. $C$ is a
list of instructions that defines the program. This syntax expresses the
following idea: ``if we are in a state where $P$ is verified, after
executing $C$ and if $C$ terminates, then $Q$ is verified for the
new state of the execution''. Put in another way, $P$ is a sufficient
precondition to ensure that $C$ will bring us to the postcondition
$Q$. For example, the Hoare triples that corresponds to the skip
action is the following one:




\begin{center}
$\{P\}$ \textbf{skip} $\{P\}$
\end{center}


When we do nothing, the postcondition is the precondition.



Along this tutorial, we will present the semantics of different program
constructs (conditional blocks, loops, etc) using Hoare logic. So,
let us skip these details now since we will work on it later. It is
not necessary to memorize these notions nor to understand all the
theoretical background, but it is still useful to have some ideas about
the way our tool works.



All of this gives us the basics that allow us to say ``here is what
this action does'' but it does not give us anything to mechanize a
proof. The tool we will use rely on a technique called weakest
precondition calculus.



\levelThreeTitle{Weakest precondition calculus}


The weakest precondition calculus is a form of predicate transform
semantics proposed by Dijkstra in 1975 in \emph{Guarded commands,
non-determinacy and formal derivation of programs}.

This title can appear complex but actually the content of the article is in
fact quite simple. We have seen before that Hoare logic gives us rules that
explain the behavior of the different actions of a program, but it does not say
how to apply these rules to establish a complete proof of the program.

Dijkstra reformulate the Hoare logic by explaining, in the triple
$\{P\}\ C\ \{Q\}$, how the instruction, or the block of instructions,
$C$ transforms the predicate $P$ in $Q$. This kind of reasoning is
called \emph{forward-reasoning}. We calculate from the precondition and
from one or multiple instructions, the strongest postcondition we can
reach. Informally, considering what we have in input, we calculate what
we will get in output. If the postcondition we want is as strong or
weaker, then we prove that there are no unexpected behaviors.



For example:



\begin{CodeBlock}{c}
int a = 2;
a = 4;
//calculated postcondition: a == 4
//expected postcondition  : 0 <= a <= 30
\end{CodeBlock}



Ok, 4 is an allowed value for \texttt{a}.



The form of predicate transformer semantics which we are interested in
works the opposite way, we speak about \emph{backward-reasoning}. From
the wanted postcondition and the instructions we are reasoning about, we
find the weakest precondition that ensures this behavior. If our actual
precondition is at least as strong, that is to say, if it implies the
computed precondition, then our program is correct.



For example, if we have the instruction:



$\{P\}$ $x$ $:=$ a $\{x = 42\}$



What is the weakest precondition to validate the postcondition
$\{x = 42\}$ ? The rule will define that $P$ is $\{$a $=42\}$.



For now, let us forget about it, we will come back to these notions as
we use them in this tutorial to understand how our tools work. So now,
we can have a look at these tools.
